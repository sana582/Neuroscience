{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b74cf0f9",
   "metadata": {},
   "source": [
    "## Apprentissage par renforcement : premiers pas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ce842",
   "metadata": {},
   "source": [
    "Classe Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "055787e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.state=0\n",
    "        self.steps_left=10\n",
    "    def get_observation(self):\n",
    "        return self.state\n",
    "    def get_actions(self): \n",
    "        return [0,1]\n",
    "    def is_done(self): \n",
    "        return self.steps_left==0\n",
    "    def action(self,action):\n",
    "        if self.is_done(): \n",
    "            raise Exception('Game over')\n",
    "        self.steps_left-=1\n",
    "        return random.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb9d17f",
   "metadata": {},
   "source": [
    "La classe Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d53f75e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "    def step(self, env):\n",
    "        obs = env.get_observation()\n",
    "        acts = env.get_actions()\n",
    "        r = env.action(random.choice(acts))\n",
    "        self.total_reward += r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e065999",
   "metadata": {},
   "source": [
    "Programme principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7ef18106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ba80bab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward got: 4.9676\n"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "agent = Agent()\n",
    " \n",
    "while not env.is_done():\n",
    "    agent.step(env)\n",
    " \n",
    "print(\"Total reward got: %.4f\" % agent.total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6666257e",
   "metadata": {},
   "source": [
    "A faire:\n",
    "Faites tourner cet environnement plusieurs fois et vérifiez que la somme des récompenses obtenues varie à chaque fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "365c6451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total reward got = 4.977578621191341\n",
      "Episode 2: Total reward got = 5.319247567021699\n",
      "Episode 3: Total reward got = 4.3696162576495645\n",
      "Episode 4: Total reward got = 3.1760365822739245\n",
      "Episode 5: Total reward got = 6.414208460981445\n",
      "Episode 6: Total reward got = 5.565690446269658\n",
      "Episode 7: Total reward got = 3.792719864949242\n",
      "Episode 8: Total reward got = 4.964623612256666\n",
      "Episode 9: Total reward got = 5.0030190514449995\n",
      "Episode 10: Total reward got = 4.030457801695413\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    env = Environment()\n",
    "    ag = Agent()\n",
    "    while not env.is_done():\n",
    "        ag.step(env)\n",
    "    print(f\"Episode {i+1}: Total reward got = {ag.total_reward}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab35f46",
   "metadata": {},
   "source": [
    "-->la somme des récompenses obtenues varie à chaque fois."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16687782",
   "metadata": {},
   "source": [
    "## Simulation d'un Monde-grille"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef80b89e",
   "metadata": {},
   "source": [
    "L'Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ae12bb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        #question 1\n",
    "        self.next = {\n",
    "            1:{\"S\":6,\"E\":2},2:{\"W\":1,\"E\":3},3:{\"W\":2,\"S\":7,\"E\":4},\n",
    "            4:{\"W\":3,\"E\":5},5:{\"W\":4,\"S\":8},6:{\"N\":1},7:{\"N\":3},8:{\"N\":5}\n",
    "        }\n",
    "        self.reward = {1:0,2:0,3:0,4:0,5:0,6:-10,7:10,8:-10}\n",
    "        #question2\n",
    "        self.state =random.randint(1,5)\n",
    "        self.steps_left = 10\n",
    "\n",
    "    def get_observation(self):\n",
    "        return self.state\n",
    "    #question3\n",
    "    def get_actions(self):\n",
    "        return list(self.next[self.state].keys())\n",
    "\n",
    "    def is_done(self):\n",
    "        return self.steps_left == 0\n",
    "\n",
    "    def action(self, a):\n",
    "        if self.is_done():\n",
    "            raise Exception(\"Game over\")\n",
    "        self.state = self.next[self.state][a]\n",
    "        self.steps_left -= 1\n",
    "        return self.reward[self.state]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8ba0c3",
   "metadata": {},
   "source": [
    "L'agent(reste le meme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "aff4d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "    def step(self, env):\n",
    "        obs = env.get_observation()\n",
    "        acts = env.get_actions()\n",
    "        r = env.action(random.choice(acts))\n",
    "        self.total_reward += r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c30727",
   "metadata": {},
   "source": [
    "Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "22d00d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a096fa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Épisode1\n",
      "Total reward got = -20.0\n",
      "\n",
      "Épisode2\n",
      "Total reward got = 30.0\n",
      "\n",
      "Épisode3\n",
      "Total reward got = -10.0\n",
      "\n",
      "Épisode4\n",
      "Total reward got = -20.0\n",
      "\n",
      "Épisode5\n",
      "Total reward got = -30.0\n",
      "\n",
      "Épisode6\n",
      "Total reward got = 10.0\n",
      "\n",
      "Épisode7\n",
      "Total reward got = -20.0\n",
      "\n",
      "Épisode8\n",
      "Total reward got = -20.0\n",
      "\n",
      "Épisode9\n",
      "Total reward got = -40.0\n",
      "\n",
      "Épisode10\n",
      "Total reward got = 20.0\n",
      "\n",
      "Épisode11\n",
      "Total reward got = -30.0\n",
      "\n",
      "Épisode12\n",
      "Total reward got = -20.0\n",
      "\n",
      "Épisode13\n",
      "Total reward got = -20.0\n",
      "\n",
      "Épisode14\n",
      "Total reward got = -10.0\n",
      "\n",
      "Épisode15\n",
      "Total reward got = -10.0\n",
      "\n",
      "Épisode16\n",
      "Total reward got = 10.0\n",
      "\n",
      "Épisode17\n",
      "Total reward got = -30.0\n",
      "\n",
      "Épisode18\n",
      "Total reward got = -10.0\n",
      "\n",
      "Épisode19\n",
      "Total reward got = -30.0\n",
      "\n",
      "Épisode20\n",
      "Total reward got = -10.0\n",
      "\n",
      "Épisode21\n",
      "Total reward got = -50.0\n",
      "\n",
      "Épisode22\n",
      "Total reward got = -30.0\n",
      "\n",
      "Épisode23\n",
      "Total reward got = 0.0\n",
      "\n",
      "Épisode24\n",
      "Total reward got = 10.0\n",
      "\n",
      "Épisode25\n",
      "Total reward got = 0.0\n",
      "\n",
      "Épisode26\n",
      "Total reward got = 10.0\n",
      "\n",
      "Épisode27\n",
      "Total reward got = 10.0\n",
      "\n",
      "Épisode28\n",
      "Total reward got = 0.0\n",
      "\n",
      "Épisode29\n",
      "Total reward got = -30.0\n",
      "\n",
      "Épisode30\n",
      "Total reward got = 0.0\n",
      "\n",
      "Épisode31\n",
      "Total reward got = -20.0\n",
      "\n",
      "Épisode32\n",
      "Total reward got = 10.0\n",
      "\n",
      "Épisode33\n",
      "Total reward got = -20.0\n",
      "\n",
      "Épisode34\n",
      "Total reward got = 20.0\n",
      "\n",
      "Épisode35\n",
      "Total reward got = 20.0\n",
      "\n",
      "Épisode36\n",
      "Total reward got = -10.0\n",
      "\n",
      "Épisode37\n",
      "Total reward got = -50.0\n",
      "\n",
      "Épisode38\n",
      "Total reward got = 0.0\n",
      "\n",
      "Épisode39\n",
      "Total reward got = 10.0\n",
      "\n",
      "Épisode40\n",
      "Total reward got = -30.0\n",
      "\n",
      "Épisode41\n",
      "Total reward got = 40.0\n",
      "\n",
      "Épisode42\n",
      "Total reward got = 20.0\n",
      "\n",
      "Épisode43\n",
      "Total reward got = 0.0\n",
      "\n",
      "Épisode44\n",
      "Total reward got = -20.0\n",
      "\n",
      "Épisode45\n",
      "Total reward got = 0.0\n",
      "\n",
      "Épisode46\n",
      "Total reward got = 0.0\n",
      "\n",
      "Épisode47\n",
      "Total reward got = 10.0\n",
      "\n",
      "Épisode48\n",
      "Total reward got = 10.0\n",
      "\n",
      "Épisode49\n",
      "Total reward got = 10.0\n",
      "\n",
      "Épisode50\n",
      "Total reward got = -20.0\n",
      "\n",
      "Épisode51\n",
      "Total reward got = 20.0\n",
      "\n",
      "Épisode52\n",
      "Total reward got = -10.0\n",
      "\n",
      "Épisode53\n",
      "Total reward got = 10.0\n",
      "\n",
      "Épisode54\n",
      "Total reward got = -20.0\n",
      "\n",
      "Épisode55\n",
      "Total reward got = 30.0\n",
      "\n",
      "Épisode56\n",
      "Total reward got = 10.0\n",
      "\n",
      "Épisode57\n",
      "Total reward got = -10.0\n",
      "\n",
      "Épisode58\n",
      "Total reward got = -10.0\n",
      "\n",
      "Épisode59\n",
      "Total reward got = 0.0\n",
      "\n",
      "Épisode60\n",
      "Total reward got = -10.0\n",
      "\n",
      "Épisode61\n",
      "Total reward got = -10.0\n",
      "\n",
      "Épisode62\n",
      "Total reward got = 0.0\n",
      "\n",
      "Épisode63\n",
      "Total reward got = -20.0\n",
      "\n",
      "Épisode64\n",
      "Total reward got = 0.0\n",
      "\n",
      "Épisode65\n",
      "Total reward got = 30.0\n",
      "\n",
      "Épisode66\n",
      "Total reward got = 0.0\n",
      "\n",
      "Épisode67\n",
      "Total reward got = -20.0\n",
      "\n",
      "Épisode68\n",
      "Total reward got = -10.0\n",
      "\n",
      "Épisode69\n",
      "Total reward got = -40.0\n",
      "\n",
      "Épisode70\n",
      "Total reward got = -10.0\n",
      "\n",
      "Épisode71\n",
      "Total reward got = 0.0\n",
      "\n",
      "Épisode72\n",
      "Total reward got = -10.0\n",
      "\n",
      "Épisode73\n",
      "Total reward got = 30.0\n",
      "\n",
      "Épisode74\n",
      "Total reward got = -10.0\n",
      "\n",
      "Épisode75\n",
      "Total reward got = 30.0\n",
      "\n",
      "Épisode76\n",
      "Total reward got = -40.0\n",
      "\n",
      "Épisode77\n",
      "Total reward got = 30.0\n",
      "\n",
      "Épisode78\n",
      "Total reward got = -10.0\n",
      "\n",
      "Épisode79\n",
      "Total reward got = -20.0\n",
      "\n",
      "Épisode80\n",
      "Total reward got = -20.0\n",
      "\n",
      "Épisode81\n",
      "Total reward got = -30.0\n",
      "\n",
      "Épisode82\n",
      "Total reward got = 0.0\n",
      "\n",
      "Épisode83\n",
      "Total reward got = -20.0\n",
      "\n",
      "Épisode84\n",
      "Total reward got = -30.0\n",
      "\n",
      "Épisode85\n",
      "Total reward got = -10.0\n",
      "\n",
      "Épisode86\n",
      "Total reward got = -40.0\n",
      "\n",
      "Épisode87\n",
      "Total reward got = -10.0\n",
      "\n",
      "Épisode88\n",
      "Total reward got = 20.0\n",
      "\n",
      "Épisode89\n",
      "Total reward got = -30.0\n",
      "\n",
      "Épisode90\n",
      "Total reward got = 0.0\n",
      "\n",
      "Épisode91\n",
      "Total reward got = -30.0\n",
      "\n",
      "Épisode92\n",
      "Total reward got = 10.0\n",
      "\n",
      "Épisode93\n",
      "Total reward got = -40.0\n",
      "\n",
      "Épisode94\n",
      "Total reward got = 0.0\n",
      "\n",
      "Épisode95\n",
      "Total reward got = -20.0\n",
      "\n",
      "Épisode96\n",
      "Total reward got = 20.0\n",
      "\n",
      "Épisode97\n",
      "Total reward got = 0.0\n",
      "\n",
      "Épisode98\n",
      "Total reward got = 0.0\n",
      "\n",
      "Épisode99\n",
      "Total reward got = -10.0\n",
      "\n",
      "Épisode100\n",
      "Total reward got = -40.0\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(100):\n",
    "    env = Environment()\n",
    "    ag = Agent()\n",
    "    print(f\"\\nÉpisode{i+1}\")\n",
    "    while not env.is_done():\n",
    "        ag.step(env)\n",
    "    print(f\"Total reward got = {ag.total_reward}\")\n",
    "    results.append(ag.total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c5ab4f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats globaux\n",
      "Moyenne des récompenses: -7.10\n",
      "Écart-type des récompenses: 19.81\n"
     ]
    }
   ],
   "source": [
    "# Calcul de la moyenne et de l’écart-type\n",
    "mean_reward = np.mean(results)\n",
    "std_reward = np.std(results)\n",
    "print(\"Résultats globaux\")\n",
    "print(f\"Moyenne des récompenses: {mean_reward:.2f}\")\n",
    "print(f\"Écart-type des récompenses: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6e631e",
   "metadata": {},
   "source": [
    "Calcul d'une fonction de valeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "22de5595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "        self.trace = [] \n",
    "    def step(self, env):\n",
    "        obs = env.get_observation()\n",
    "        acts = env.get_actions()\n",
    "        chosen_action = random.choice(acts)\n",
    "        reward = env.action(chosen_action)\n",
    "        self.total_reward += reward\n",
    "        self.trace.append(obs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3c5297a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100  \n",
    "V = {1 :0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0} \n",
    "nb_visites = {1 :0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0} \n",
    "total_reward_sum = {1 :0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0} \n",
    "for i in range(N):\n",
    "    env = Environment()\n",
    "    ag = Agent()\n",
    "    while not env.is_done():\n",
    "        ag.step(env)\n",
    "    for s in ag.trace:\n",
    "        nb_visites[s] += 1\n",
    "        total_reward_sum[s] += ag.total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0a368639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonction de valeur estimée V(s)\n",
      "État 1: V(s) = -14.21 (visites=145)\n",
      "État 2: V(s) = -5.40 (visites=161)\n",
      "État 3: V(s) = 1.41 (visites=205)\n",
      "État 4: V(s) = -5.96 (visites=151)\n",
      "État 5: V(s) = -16.53 (visites=150)\n",
      "État 6: V(s) = -21.54 (visites=65)\n",
      "État 7: V(s) = 11.40 (visites=50)\n",
      "État 8: V(s) = -22.60 (visites=73)\n"
     ]
    }
   ],
   "source": [
    "for s in V:\n",
    "    if nb_visites[s] > 0:\n",
    "        V[s] = total_reward_sum[s] / nb_visites[s]\n",
    "print(\"Fonction de valeur estimée V(s)\")\n",
    "for s in V:\n",
    "    print(f\"État {s}: V(s) = {V[s]:.2f} (visites={nb_visites[s]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf53c4",
   "metadata": {},
   "source": [
    "Politique guidée par la valeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b6ff1394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "        self.trace = []\n",
    "\n",
    "    def step(self, env):\n",
    "        obs = env.get_observation()\n",
    "        acts = env.get_actions()\n",
    "        chosen_action = random.choice(acts)\n",
    "        reward = env.action(chosen_action)\n",
    "        self.total_reward += reward\n",
    "        self.trace.append(obs)\n",
    "\n",
    "    def step_valeur(self, env, V):\n",
    "        obs = env.get_observation()\n",
    "        acts = env.get_actions()\n",
    "        best_action = max(acts, key=lambda a: V[env.next[obs][a]])\n",
    "        reward = env.action(best_action)\n",
    "        self.total_reward += reward\n",
    "        self.trace.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "da4e91b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politique simple:\n",
      "Moyenne des récompenses: -5.90\n",
      "Écart-type: 21.55\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "results_random = []\n",
    "for i in range(N):\n",
    "    env = Environment()\n",
    "    ag = Agent()\n",
    "    while not env.is_done():\n",
    "        ag.step(env)\n",
    "    results_random.append(ag.total_reward)\n",
    "\n",
    "mean_random = np.mean(results_random)\n",
    "std_random = np.std(results_random)\n",
    "\n",
    "print(\"Politique simple:\")\n",
    "print(f\"Moyenne des récompenses: {mean_random:.2f}\")\n",
    "print(f\"Écart-type: {std_random:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9e5fb588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Politique guidée par la valeur:\n",
      "Moyenne des récompenses: 46.40\n",
      "Écart-type: 4.80\n"
     ]
    }
   ],
   "source": [
    "results_valeur = []\n",
    "for i in range(N):\n",
    "    env = Environment()\n",
    "    ag = Agent()\n",
    "    while not env.is_done():\n",
    "        ag.step_valeur(env, V)\n",
    "    results_valeur.append(ag.total_reward)\n",
    "\n",
    "mean_valeur = np.mean(results_valeur)\n",
    "std_valeur = np.std(results_valeur)\n",
    "\n",
    "print(\"\\nPolitique guidée par la valeur:\")\n",
    "print(f\"Moyenne des récompenses: {mean_valeur:.2f}\")\n",
    "print(f\"Écart-type: {std_valeur:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d472c",
   "metadata": {},
   "source": [
    "la comparaison des valeurs :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860535a1",
   "metadata": {},
   "source": [
    "Avec la politique aléatoire, l’agent obtient en moyenne des récompenses négatives et très variables, car il tombe souvent dans les pièges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e593e7b7",
   "metadata": {},
   "source": [
    "Avec la politique guidée par la valeur, l’agent maximise ses gains en évitant les états défavorables et en ciblant les récompenses positives, ce qui donne une moyenne élevée et stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c55b897",
   "metadata": {},
   "source": [
    "Améliorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dce07c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.rewards = [] \n",
    "        self.trace = []     \n",
    "    def step(self, env):\n",
    "        obs = env.get_observation()\n",
    "        acts = env.get_actions()\n",
    "        chosen_action = random.choice(acts)\n",
    "        reward = env.action(chosen_action)\n",
    "        self.rewards.append(reward)\n",
    "        self.trace.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "926d1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "V = {s:0 for s in range(1,9)}\n",
    "nb_visites = {s:0 for s in range(1,9)}\n",
    "total_reward_sum = {s:0 for s in range(1,9)}\n",
    "\n",
    "for i in range(N):\n",
    "    env = Environment()\n",
    "    ag = Agent()\n",
    "    while not env.is_done():\n",
    "        ag.step(env)\n",
    "    for idx, s in enumerate(ag.trace):\n",
    "        future_reward_sum = sum(ag.rewards[idx:])\n",
    "        total_reward_sum[s] += future_reward_sum\n",
    "        nb_visites[s] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed865d",
   "metadata": {},
   "source": [
    "calcul de V(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a2b79c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valeur de V(s)\n",
      "État 1: V(s) = -10.49 (visites=164)\n",
      "État 2: V(s) = -2.60 (visites=146)\n",
      "État 3: V(s) = 2.89 (visites=204)\n",
      "État 4: V(s) = -2.57 (visites=140)\n",
      "État 5: V(s) = -7.35 (visites=132)\n",
      "État 6: V(s) = -7.98 (visites=84)\n",
      "État 7: V(s) = 2.97 (visites=74)\n",
      "État 8: V(s) = -5.00 (visites=56)\n"
     ]
    }
   ],
   "source": [
    "for s in V:\n",
    "    if nb_visites[s] > 0:\n",
    "        V[s] = total_reward_sum[s] / nb_visites[s]\n",
    "\n",
    "print(\"valeur de V(s)\")\n",
    "for s in V:\n",
    "    print(f\"État {s}: V(s) = {V[s]:.2f} (visites={nb_visites[s]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c085243e",
   "metadata": {},
   "source": [
    "comparaisons des valeurs :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a52bab",
   "metadata": {},
   "source": [
    "Ancienne méthode : chaque état recevait la récompense totale de l’épisode --> estimation grossière et peu précise.\n",
    "\n",
    "Nouvelle méthode : chaque état prend en compte les récompenses présentes et futures --> estimation plus fine et réaliste."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
